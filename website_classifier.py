# -*- coding: utf-8 -*-
"""website_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a0MVhFCSRSLfWlDU6hyIvoXlXgIzZbOc
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import keras
import sklearn.model_selection
import sklearn.preprocessing
import string
from tensorflow.keras.layers import TextVectorization
from bs4 import BeautifulSoup
import requests
from joblib import Parallel,delayed
import math

website_categories = pd.read_csv('/content/websiteCateegories.csv')

website_categories

stopwords = [ "a", "about", "above", "after", "again", "against", "all", "am", "an", "and", "any", "are", "as", "at", "be", "because",
             "been", "before", "being", "below", "between", "both", "but", "by", "could", "did", "do", "does", "doing", "down", "during",
             "each", "few", "for", "from", "further", "had", "has", "have", "having", "he", "he'd", "he'll", "he's", "her", "here",
             "here's", "hers", "herself", "him", "himself", "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into",
             "is", "it", "it's", "its", "itself", "let's", "me", "more", "most", "my", "myself", "nor", "of", "on", "once", "only", "or",
             "other", "ought", "our", "ours", "ourselves", "out", "over", "own", "same", "she", "she'd", "she'll", "she's", "should",
             "so", "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's",
             "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until", "up",
             "very", "was", "we", "we'd", "we'll", "we're", "we've", "were", "what", "what's", "when", "when's", "where", "where's",
             "which", "while", "who", "who's", "whom", "why", "why's", "with", "would", "you", "you'd", "you'll", "you're", "you've",
             "your", "yours", "yourself", "yourselves"]

values = pd.read_csv('/content/value_counts.csv')

#preprocessing functions
#take our titles and remove stopwords
def remove_stopwords(data):
  data['title'] = data['title'].apply(lambda x : ' '.join([word for word in str(x).split() if word not in (stopwords)]))

def remove_punctuation(test_str):
# Using filter() and lambda function to filter out punctuation characters
  result = ''.join(filter(lambda x: x.isalpha() or x.isdigit() or x.isspace(), test_str))
  return result
def reduce_labels(label):
  ans = ''
  for char in str(label)[1:]:
    if char=='/':
      return ans
    else:
      ans+=char
  return ans
def apply_labels(val):
  return label_encodings[val]
def remove_punctuation(s):
  return s.translate(str.maketrans('', '', string.punctuation))

# removes all the stopworkds from our csv file
remove_stopwords(website_categories)
website_categories['category'] = website_categories['category'].apply(reduce_labels)
website_categories['title'] = website_categories['title'].apply(remove_punctuation)

categories = website_categories['category'].unique()
class_names = {}
for i,category in enumerate(categories):
  class_names[category] = i
class_names


def apply_encodings(category):
  return class_names[category]

website_categories['category'].value_counts()

#inverse map from encodings to classnames
inv_map = {v: k for k, v in class_names.items()}

value_counts = pd.read_csv('/content/value_counts.csv')

value_counts

#I am going to do top 9
top_9_categories = ['Business & Industrial','Arts & Entertainment','Shopping','Internet & Telecom','Hobbies & Leisure','Computers & Electronics','People & Society','Home & Garden','Health' ]

#filter only include the rows in
top_9 = website_categories.loc[website_categories['category'].isin(top_9_categories)]

#I can simply just add url here
top_9 = top_9[['category','title']]

#balancing out, also can modify this to get urls as well
max_length = 1013
top_9_counts = {}
category = []
title = []


for index, row in top_9.iterrows():

  if row['category'] not in top_9_counts.keys():
    top_9_counts[row['category']] = 0
    title.append(row['title'])
    category.append(row['category'])


  elif top_9_counts[row['category']]<max_length:
    top_9_counts[row['category']]+=1
    title.append(row['title'])
    category.append(row['category'])

final_top_9 = pd.DataFrame()
final_top_9['category'] = category
final_top_9['title'] = title

encoded_categories = final_top_9['category'].apply(apply_encodings)

X_train, X_test,Y_train, Y_test = sklearn.model_selection.train_test_split(final_top_9['title'], encoded_categories, test_size=0.2, random_state = 45)
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 50000)
tokenizer.fit_on_texts(X_train)
words_to_index = tokenizer.word_index

path_to_glove_file = '/content/glove.6B.50d.txt'

embeddings_index = {}
with open(path_to_glove_file) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        embeddings_index[word] = coefs

print("Found %s word vectors." % len(embeddings_index))

vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=25)
vectorizer.adapt(final_top_9['title'])
x_train = vectorizer(np.array([[s] for s in X_train])).numpy()
x_val = vectorizer(np.array([[s] for s in X_test])).numpy()
y_train = np.array(Y_train)
y_val = np.array(Y_test)

num_tokens = len(words_to_index) + 2
embedding_dim = 50
hits = 0
misses = 0

# Prepare embedding matrix
embedding_matrix = np.zeros((num_tokens, embedding_dim))
for word, i in words_to_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in embedding index will be all-zeros.
        # This includes the representation for "padding" and "OOV"
        embedding_matrix[i] = embedding_vector
        hits += 1
    else:
        misses += 1
print("Converted %d words (%d misses)" % (hits, misses))

embedding_layer = tf.keras.layers.Embedding(
    num_tokens,
    embedding_dim,
    embeddings_initializer=keras.initializers.Constant(embedding_matrix),
    trainable=False,
)

int_sequences_input = keras.Input(shape=(None,), dtype="int64")

embedded_sequences = embedding_layer(int_sequences_input)
x = tf.keras.layers.Conv1D(128, 5, activation="relu")(embedded_sequences)
x = tf.keras.layers.Dropout(0.25)(x)
x = tf.keras.layers.Conv1D(128, 5, activation="relu")(x)
x = tf.keras.layers.Dropout(.2)(x)
x = tf.keras.layers.GlobalMaxPooling1D()(x)

x = tf.keras.layers.Dense(100, activation="relu")(x)

preds = tf.keras.layers.Dense(len(class_names), activation="softmax")(x)
model = tf.keras.Model(int_sequences_input, preds)
model.summary()

model.compile(
    loss="sparse_categorical_crossentropy", optimizer=tf.keras.optimizers.Adam(0.001), metrics=["accuracy"]
)

model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))

def predict_using_model(model, phrase, inv_map):
  string_input = keras.Input(shape=(1,), dtype="string")
  x = vectorizer(string_input)
  preds = model(x)
  end_to_end_model = keras.Model(string_input, preds)

  probabilities = end_to_end_model.predict(
    [[phrase]]
  )


  high = np.argmax(probabilities)

  return inv_map[high]

predict_using_model(model, 'friends', inv_map)

def returntitle(url):
  try:
    request = requests.get(url)
  except:
    return 'error'
  Soup = BeautifulSoup(request.text, 'lxml')
  title_tag = Soup.find('title')
  if title_tag:
    title_text = title_tag.text
  return title_text

def categorize(url):
  title = returntitle(url)
  category = predict_using_model(model, title, inv_map)
  return category

categorize('https://myanimelist.net/')

